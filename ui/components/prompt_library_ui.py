"""
Prompt Library UI components for CodeInsight.

This module provides a reusable UI component for displaying and managing prompts
generated by the swarm analysis system.
"""

import streamlit as st
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging

from utils.prompts.prompt_storage import (
    get_all_prompts,
    get_prompt_statistics
)

logger = logging.getLogger(__name__)


def _format_timestamp(timestamp: Any, format_str: str = "%Y-%m-%d %H:%M:%S") -> str:
    """
    Safely format a timestamp for display.
    
    Args:
        timestamp: Can be a datetime object, string, or None
        format_str: Format string for datetime objects (default: ISO-like format)
    
    Returns:
        Formatted timestamp string, or 'Unknown' if timestamp is missing/invalid
    """
    if timestamp is None:
        return 'Unknown'
    
    if isinstance(timestamp, datetime):
        return timestamp.strftime(format_str)
    elif isinstance(timestamp, str):
        # Try to parse if it's an ISO format string
        try:
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            return dt.strftime(format_str)
        except (ValueError, AttributeError):
            return timestamp
    else:
        try:
            return str(timestamp)
        except Exception:
            return 'Unknown'


def _check_database_health() -> tuple[bool, str]:
    """Check if the prompt database is accessible."""
    try:
        from services.db_config import get_db_config, CODELUMEN_DATABASE
        from services.postgresql_connection_pool import get_db_connection
        
        config = get_db_config()
        schema = config.postgresql.schema
        
        # Try to connect and query
        with get_db_connection(CODELUMEN_DATABASE, read_only=True) as conn:
            with conn.cursor() as cursor:
                cursor.execute(f"SELECT COUNT(*) FROM {schema}.prompts")
                count = cursor.fetchone()[0] or 0
            return True, f"Database accessible ({count} prompts found)"
    except Exception as e:
        logger.error(f"Database health check failed: {e}", exc_info=True)
        return False, f"Database error: {str(e)}"


def render_prompt_library_tab():
    """Render the Prompt Library tab."""
    st.header("ðŸ“š Prompt Library")
    st.markdown("View and manage prompts generated by the swarm analysis system.")
    
    # Database health check
    db_healthy, db_message = _check_database_health()
    if not db_healthy:
        st.warning(f"âš ï¸ {db_message}")
        with st.expander("Database Information", expanded=False):
            try:
                from services.db_config import get_db_config
                config = get_db_config()
                st.code(f"Database: PostgreSQL (codelumen)\nSchema: {config.postgresql.schema}", language="text")
            except Exception as e:
                st.code(f"Database: PostgreSQL (codelumen)\nError: {str(e)}", language="text")
    else:
        st.success(f"âœ… {db_message}")
    
    st.divider()
    
    # Load prompts
    try:
        prompts = get_all_prompts(limit=100)
        logger.debug(f"Loaded {len(prompts)} prompts from storage")
    except Exception as e:
        logger.error(f"Error loading prompts: {e}", exc_info=True)
        from utils.error_formatting import format_user_error
        st.error(f"âŒ Error loading prompts from database: {format_user_error(e)}")
        st.info("Please check the logs for more details.")
        # Try to show more debug info
        with st.expander("Debug Information", expanded=False):
            try:
                from services.storage.cached_prompt_storage import CachedPromptStorage
                storage = CachedPromptStorage()
                # Try direct query
                direct_prompts = storage.storage.get_all_prompts(limit=100)
                st.write(f"Direct storage query returned: {len(direct_prompts)} prompts")
                if direct_prompts:
                    st.json(direct_prompts[0])
            except Exception as debug_e:
                st.write(f"Debug query failed: {debug_e}")
        return
    
    st.caption(f"Prompts returned from database: {len(prompts)}")
    
    if not prompts:
        st.info("â„¹ï¸ No prompts found in the library. Run a swarm analysis to generate prompts.")
        with st.expander("Debug: Raw DB query result", expanded=False):
            st.write("Empty list returned from get_all_prompts(limit=100).")
        return
    
    # Search and filter section
    st.subheader("ðŸ” Search & Filter")
    
    # Search box
    search_query = st.text_input(
        "Search prompts",
        placeholder="Search by name, description, or content...",
        key="prompt_search"
    )
    
    # Filter options
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Get unique roles from prompts
        roles = set()
        for p in prompts:
            # Try prompt_name first, then role field
            name = p.get("prompt_name", "") or p.get("role", "")
            if name:
                # Extract role name (before any " - " separator)
                role = name.split(" - ")[0].strip()
                if role:
                    roles.add(role)
            # Also check role field directly
            role_field = p.get("role")
            if role_field:
                roles.add(role_field)
        
        filter_role = st.selectbox(
            "Filter by Role",
            options=["All"] + sorted(list(roles)),
            index=0,
            key="filter_role"
        )
    
    with col2:
        filter_optimized = st.selectbox(
            "Filter by Optimization",
            options=["All", "Optimized", "Not Optimized"],
            index=0,
            key="filter_optimized"
        )
    
    with col3:
        filter_validated = st.selectbox(
            "Filter by Validation",
            options=["All", "Validated", "Not Validated"],
            index=0,
            key="filter_validated"
        )
    
    # Apply filters
    filtered_prompts = prompts
    
    # Search filter
    if search_query:
        search_lower = search_query.lower()
        filtered_prompts = [
            p for p in filtered_prompts
            if (search_lower in p.get("prompt_name", "").lower() or
                search_lower in p.get("prompt_content", "").lower() or
                search_lower in str(p.get("metadata", {})).lower())
        ]
    
    # Role filter
    if filter_role != "All":
        filtered_prompts = [
            p for p in filtered_prompts
            if (p.get("prompt_name", "").startswith(filter_role) or
                p.get("role") == filter_role)
        ]
    
    # Optimization filter
    if filter_optimized == "Optimized":
        filtered_prompts = [
            p for p in filtered_prompts
            if p.get("metadata", {}).get("optimized", False)
        ]
    elif filter_optimized == "Not Optimized":
        filtered_prompts = [
            p for p in filtered_prompts
            if not p.get("metadata", {}).get("optimized", False)
        ]
    
    # Validation filter
    if filter_validated == "Validated":
        filtered_prompts = [
            p for p in filtered_prompts
            if p.get("metadata", {}).get("validated", False)
        ]
    elif filter_validated == "Not Validated":
        filtered_prompts = [
            p for p in filtered_prompts
            if not p.get("metadata", {}).get("validated", False)
        ]
    
    # Display summary
    st.info(f"ðŸ“Š Showing {len(filtered_prompts)} of {len(prompts)} prompts")
    
    if not filtered_prompts:
        st.warning("No prompts match your search/filter criteria.")
        return
    
    st.divider()
    
    # Optional debug: show first few raw prompt records to verify mapping
    with st.expander("Debug: Raw prompt records from PostgreSQL", expanded=False):
        st.json(filtered_prompts[:5])
    
    # Display prompts
    for idx, prompt in enumerate(filtered_prompts):
        metadata = prompt.get("metadata", {})
        is_validated = metadata.get("validated", False)
        confidence = metadata.get("confidence", None)
        
        # Build title
        name = prompt.get("prompt_name", "Unnamed Prompt")
        title = f"ðŸ“ {name}"
        
        # Add confidence if validated
        if is_validated and confidence is not None:
            title += f" (Confidence: {confidence:.2f})"
        
        # Add timestamp
        created_at = _format_timestamp(prompt.get("created_at"))
        title += f" - {created_at}"
        
        with st.expander(title, expanded=False):
            # Description (may be in metadata)
            description = metadata.get("description") or prompt.get("description", "No description available")
            if description and description != "N/A":
                st.markdown(f"**Description:** {description}")
            
            # Metadata badges
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                if metadata.get("optimized", False):
                    st.success("âœ… Optimized")
                else:
                    st.info("â„¹ï¸ Not Optimized")
            
            with col2:
                if metadata.get("llm_generated", False):
                    st.success("âœ… LLM Generated")
                else:
                    st.info("â„¹ï¸ Template Based")
            
            with col3:
                if is_validated:
                    if confidence is not None:
                        st.success(f"âœ… Validated ({confidence:.2f})")
                    else:
                        st.success("âœ… Validated")
                else:
                    st.warning("âš ï¸ Not Validated")
            
            with col4:
                version = metadata.get("version", 1)
                st.caption(f"Version: {version}")
            
            # Tags (may be in metadata)
            tags = metadata.get("tags", [])
            if not tags and isinstance(prompt.get("tags"), list):
                tags = prompt.get("tags", [])
            if tags and isinstance(tags, list) and len(tags) > 0:
                st.markdown("**Tags:**")
                tag_str = ", ".join([f"`{tag}`" for tag in tags if tag])
                st.markdown(tag_str)
            
            # Role
            role = prompt.get("role")
            if role:
                st.markdown(f"**Role:** `{role}`")
            
            # Architecture hash
            arch_hash = prompt.get("architecture_hash")
            if arch_hash:
                st.caption(f"**Architecture Hash:** `{arch_hash[:16]}...`")
            
            # Usage count
            usage_count = prompt.get("usage_count", 0)
            if usage_count > 0:
                st.caption(f"**Usage Count:** {usage_count}")
            
            # Prompt content
            st.markdown("**Prompt Content:**")
            content = prompt.get("prompt_content", "")
            if content:
                st.code(content, language="markdown")
            else:
                st.warning("No content available")
            
            # Metadata details (collapsible)
            if metadata:
                with st.expander("ðŸ“‹ View Full Metadata", expanded=False):
                    st.json(metadata)
            
            # Additional info
            col1, col2 = st.columns(2)
            with col1:
                updated_at = _format_timestamp(prompt.get("updated_at"))
                st.caption(f"**Updated:** {updated_at}")
            with col2:
                prompt_id = prompt.get("prompt_id", "N/A")
                st.caption(f"**ID:** {prompt_id}")

